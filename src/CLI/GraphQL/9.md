# Connecter les services d'apprentissage automatique

## @predictions

La directive `@predictions` vous permet d'interroger une orchestration de services AI/ML tels que Amazon Rekognition, Amazon Translate, et/ou Amazon Polly.

> Remarque : La prise en charge de l'ajout de la directive `@predictions` utilise le seau de stockage s3 qui est configuré via le CLI. Pour le moment, cette directive ne fonctionne qu'avec les objets situés dans `public/`.

### Definition

Les actions supportées dans cette directive sont incluses dans la définition.

```graphql
directive @predictions(actions: [PredictionsActions!]!) on FIELD_DEFINITION
enum PredictionsActions {
  identifyText # utilise Amazon Rekognition pour détecter du texte
  identifyLabels # utilise Amazon Rekognition pour détecter les étiquettes
  convertTextToSpeech # utilise Amazon Polly dans un lambda pour convertir une url présignée en parole synthétisée.
  translateText # utilise Amazon Translate pour traduire le texte de la langue source à la langue cible.
}
```

### Usage

Étant donné le schéma suivant, une opération de requête est définie pour faire ce qui suit avec l'image fournie.

- Identifier le texte de l'image
- Traduire le texte de cette image
- Synthétiser la parole à partir du texte traduit.

```graphql
type Query {
  speakTranslatedImageText: String
    @predictions(actions: [identifyText, translateText, convertTextToSpeech])
}
```

Un exemple de cette requête ressemblera à ceci :

```graphql
query SpeakTranslatedImageText($input: SpeakTranslatedImageTextInput!) {
  speakTranslatedImageText(
    input: {
      identifyText: { key: "myimage.jpg" }
      translateText: { sourceLanguage: "en", targetLanguage: "es" }
      convertTextToSpeech: { voiceID: "Conchita" }
    }
  )
}
```

Un exemple de code utilisant la bibliothèque JS :

```js
import React, { useState } from "react";
import Amplify, { Storage, API, graphqlOperation } from "aws-amplify";
import awsconfig from "./aws-exports";
import { speakTranslatedImageText } from "./graphql/queries";

/* Configure Exports */
Amplify.configure(awsconfig);

function SpeakTranslatedImage() {
  const [src, setSrc] = useState("");
  const [img, setImg] = useState("");

  function putS3Image(event) {
    const file = event.target.files[0];
    Storage.put(file.name, file)
      .then(async (result) => {
        setSrc(await speakTranslatedImageTextOP(result.key));
        setImg(await Storage.get(result.key));
      })
      .catch((err) => console.log(err));
  }

  return (
    <div className="Text">
      <div>
        <h3>Upload Image</h3>
        <input
          type="file"
          accept="image/jpeg"
          onChange={(event) => {
            putS3Image(event);
          }}
        />
        <br />
        {img && <img src={img}></img>}
        {src && (
          <div>
            {" "}
            <audio id="audioPlayback" controls>
              <source id="audioSource" type="audio/mp3" src={src} />
            </audio>{" "}
          </div>
        )}
      </div>
    </div>
  );
}

async function speakTranslatedImageTextOP(key) {
  const inputObj = {
    translateText: {
      sourceLanguage: "en",
      targetLanguage: "es",
    },
    identifyText: { key },
    convertTextToSpeech: { voiceID: "Conchita" },
  };
  const response = await API.graphql(
    graphqlOperation(speakTranslatedImageText, { input: inputObj })
  );
  return response.data.speakTranslatedImageText;
}
function App() {
  return (
    <div className="App">
      <h1>Speak Translated Image</h1>
      <SpeakTranslatedImage />
    </div>
  );
}
export default App;
copy;
```

### How it works

A partir de l'exemple de schéma ci-dessus, `@predictions` va créer des ressources pour communiquer avec Amazon Rekognition, Translate et Polly. Pour chaque action, les éléments suivants sont créés :

- Une politique IAM pour chaque service (par exemple, la politique `detectText` d'Amazon Rekognition)
- Une fonction VTL AppSync
- Une source de données AppSync

Enfin, un résolveur est créé pour `speakTranslatedImageText` qui est un résolveur pipeline composé de fonctions AppSync qui sont définies par la liste d'actions fournie dans la directive.

### Actions

Chacune des actions décrites dans la section de définition des @predictions peut être utilisée individuellement, ainsi que dans une séquence. Les séquences d'actions supportées aujourd'hui sont les suivantes :

- `identifyText -> translateText -> convertTextToSpeech`
- `identifyLabels -> translateText -> convertTextToSpeech`
- `translateText -> convertTextToSpeech`

### Action resources

- Codes de langue pris en charge par `translateText`](https://docs.aws.amazon.com/translate/latest/dg/what-is.html#what-is-languages)
- Les identifiants vocaux pris en charge par `convertTextToSpeech` (https://docs.aws.amazon.com/polly/latest/dg/voicelist.html)
